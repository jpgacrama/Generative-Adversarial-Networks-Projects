{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete Generator Adversarial Network which I typed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate this when running in Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# import os\n",
    "# drive.mount('/drive', force_remount=True)\n",
    "# os.chdir('/drive/My Drive/Colab/DeepLearning/tf_framework')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import scipy.io as io\n",
    "import numpy as np\n",
    "import scipy.ndimage as nd\n",
    "import matplotlib.pyplot as plt\n",
    "from os import system, name  \n",
    "from tensorflow.keras.backend import shape\n",
    "from tensorflow.keras.layers import Input, LeakyReLU\n",
    "from tensorflow.keras.layers import Conv3DTranspose, Conv3D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "DIR_PATH = './data/3DShapeNets/volumetric_data/'\n",
    "\n",
    "# Fixed problems with Error #15: Initializing libiomp5md.dll, but found libiomp5 already initialized.\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to clear the screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear():\n",
    "  \n",
    "    # for windows\n",
    "    if name == 'nt':\n",
    "        _ = system('cls')\n",
    "  \n",
    "    # for mac and linux(here, os.name is 'posix')\n",
    "    else:\n",
    "        _ = system('clear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to build the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    '''\n",
    "    Create a Generator Model with hyperparameters values defined as follows\n",
    "    :return: Generator network\n",
    "    '''\n",
    "    z_size = 200\n",
    "    gen_filters = [512, 256, 128, 64, 1]\n",
    "    gen_kernel_sizes = [4, 4, 4, 4, 4]\n",
    "    gen_strides = [1, 2, 2, 2, 2]\n",
    "    gen_input_shape = (1, 1, 1, z_size)\n",
    "    gen_activations = ['relu', 'relu', 'relu', 'relu', 'sigmoid']\n",
    "    gen_convolutional_blocks = 5\n",
    "\n",
    "    # Create the input layer\n",
    "    input_layer = Input(shape=gen_input_shape, name='Generator Input Layer')\n",
    "\n",
    "    # First 3D transpose convolution otherwise known in Keras as Deconvolution\n",
    "    layer = Conv3DTranspose(filters=gen_filters[0],\n",
    "                     kernel_size=gen_kernel_sizes[0],\n",
    "                     strides=gen_strides[0])(input_layer) \n",
    "    layer = BatchNormalization()(layer, training=True)\n",
    "    layer = Activation(activation=gen_activations[0])(layer)\n",
    "\n",
    "    # Add 4 3D transpose convolution blocks\n",
    "    for i in range(gen_convolutional_blocks - 1):\n",
    "        layer = Conv3DTranspose(\n",
    "                    filters=gen_filters[i+1],\n",
    "                    kernel_size=gen_kernel_sizes[i+1],\n",
    "                    strides=gen_strides[i+1],\n",
    "                    padding='same')(layer)\n",
    "        layer = BatchNormalization()(layer, training=True)\n",
    "        layer = Activation(activation=gen_activations[i+1])(layer)\n",
    "\n",
    "    # Create a Keras Model\n",
    "    gen_model = Model(inputs=[input_layer], outputs=layer)\n",
    "    gen_model.summary()\n",
    "    return gen_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to build the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    '''\n",
    "    Create a Discriminator Model using hyperparameters values defined as follows\n",
    "    :return: Discriminator network\n",
    "    '''\n",
    "    dis_input_shape = (64, 64, 64, 1)\n",
    "    dis_filters = [64, 128, 256, 512, 1]\n",
    "    dis_kernel_sizes = [4, 4, 4, 4, 4]\n",
    "    dis_strides = [2, 2, 2, 2, 1]\n",
    "    dis_paddings = ['same', 'same', 'same', 'same', 'valid']\n",
    "    dis_alphas = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "    dis_activations = ['leaky_relu', 'leaky_relu', 'leaky_relu', 'leaky_relu', 'sigmoid']\n",
    "    dis_convolutional_blocks = 5\n",
    "    \n",
    "    # Create the input layer\n",
    "    dis_input_layer = Input(shape=dis_input_shape, name='Discriminator Input Layer')\n",
    "\n",
    "    # The first 3D convolution block\n",
    "    layer = Conv3D(filters=dis_filters[0],\n",
    "                   kernel_size=dis_kernel_sizes[0],\n",
    "                   strides=dis_strides[0],\n",
    "                   padding=dis_paddings[0])(dis_input_layer)\n",
    "    layer = BatchNormalization()(layer, training=True)\n",
    "    layer = LeakyReLU(dis_alphas[0])(layer)\n",
    "\n",
    "    # Add 4 more convolutional blocks\n",
    "    for i in range(dis_convolutional_blocks - 1):\n",
    "        layer = Conv3D(filters=dis_filters[i+1],\n",
    "                       kernel_size=dis_kernel_sizes[i+1],\n",
    "                       strides=dis_strides[i+1],\n",
    "                       padding=dis_paddings[i+1])(layer)\n",
    "        layer = BatchNormalization()(layer, training=True)\n",
    "        if dis_activations[i+1] == 'leaky_relu':\n",
    "            layer = LeakyReLU(dis_alphas[i+1])(layer)\n",
    "        elif dis_activations[i + 1] == 'sigmoid':\n",
    "            layer = Activation(activation='sigmoid')(layer)\n",
    "\n",
    "    dis_model = Model(inputs=[dis_input_layer], outputs=layer)\n",
    "    dis_model.summary()\n",
    "    return dis_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to get various data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVoxelsFromMat(path, cube_len=64):\n",
    "    voxels = io.loadmat(path)['instance']\n",
    "    voxels = np.pad(voxels, (1, 1), 'constant', constant_values=(0, 0))\n",
    "    if cube_len != 32 and cube_len == 64:\n",
    "        voxels = nd.zoom(voxels, (2, 2, 2), mode='constant', order=0)\n",
    "    return voxels\n",
    "\n",
    "def get3ImagesForACategory(obj='airplane', train=True, cube_len=64, obj_ratio=1.0):\n",
    "    obj_path = DIR_PATH + obj + '/30/'\n",
    "    obj_path += 'train/' if train else 'test/'\n",
    "    fileList = [f for f in os.listdir(obj_path) if f.endswith('.mat')]\n",
    "    fileList = fileList[0:int(obj_ratio * len(fileList))]\n",
    "    volumeBatch = np.asarray([getVoxelsFromMat(obj_path + f, cube_len) for f in fileList], dtype=bool)\n",
    "    return volumeBatch\n",
    "\n",
    "def saveFromVoxels(voxels, path):\n",
    "    z, x, y = voxels.nonzero()\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(x, y, -z, zdir='z', c='red')\n",
    "    plt.savefig(path)\n",
    "\n",
    "def write_log(callback, name, value, batch_no):\n",
    "    summary = tf.Summary()\n",
    "    summary_value = summary.value.add()\n",
    "    summary_value.simple_value = value\n",
    "    summary_value.tag = name\n",
    "    callback.writer.add_summary(summary, batch_no)\n",
    "    callback.writer.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    clear() # Clears the terminal \n",
    "    \n",
    "    # Hyperparameters\n",
    "    gen_learning_rate = 0.0025\n",
    "    dis_learning_rate = 0.00001\n",
    "    gen_beta = 0.5\n",
    "    dis_beta = 0.9\n",
    "    adversarialModel_beta = 0.5\n",
    "    batch_size = 32\n",
    "    z_size = 200\n",
    "    generated_volumes_dir = 'generated_volumes'\n",
    "    log_dir = 'logs'\n",
    "    epochs = 10\n",
    "    MODE = 'train'\n",
    "    \n",
    "    # Create two lists to store losses\n",
    "    gen_losses = []\n",
    "    dis_losses = []\n",
    "\n",
    "    # Build the Generator and Discriminator\n",
    "    generator = build_generator()\n",
    "    discriminator = build_discriminator()\n",
    "\n",
    "    # Specify Optimizer\n",
    "    gen_optimizer = Adam(learning_rate=gen_learning_rate, beta_1=gen_beta)\n",
    "    dis_optimimzer = Adam(learning_rate=dis_learning_rate, beta_1=dis_beta)\n",
    "\n",
    "    # Compile networks\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=gen_optimizer)\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=dis_optimimzer)\n",
    "\n",
    "    # Create and compile the adversarial model\n",
    "    discriminator.trainable = False\n",
    "    adversarial_model = Sequential(name='AdversarialModel')\n",
    "    adversarial_model.add(generator)\n",
    "    adversarial_model.add(discriminator)\n",
    "    adversarial_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=gen_learning_rate, beta_1=adversarialModel_beta))\n",
    "    adversarial_model.summary()\n",
    "\n",
    "    # Getting images\n",
    "    volumes = get3ImagesForACategory(obj='airplane', train=True, obj_ratio=1.0)\n",
    "    volumes = volumes[..., np.newaxis].astype(float)\n",
    "\n",
    "    # Creating the Tensorflow callback class\n",
    "    tensorboard = TensorBoard(log_dir='{}/{}'.format(log_dir, time.time()))\n",
    "    tensorboard.set_model(generator)\n",
    "    tensorboard.set_model(discriminator)\n",
    "\n",
    "    # Run the simulation for a specified number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nEpoch: {epoch}')\n",
    "\n",
    "        number_of_batches = int(volumes.shape[0] / batch_size)\n",
    "        print(f'Number of batches: {number_of_batches}')\n",
    "        \n",
    "        for index in range(number_of_batches):\n",
    "            print(f'Batch: {index + 1}')\n",
    "\n",
    "            z_sample = np.random.normal(0, 0.33, size=[batch_size, 1, 1, 1, z_size]).astype(np.float32)\n",
    "            volumes_batch = volumes[index * batch_size:(index + 1) * batch_size, :, :, :]\n",
    "\n",
    "            # Generate Fake images'\n",
    "            gen_volumes = generator.predict(z_sample,verbose=3)\n",
    "\n",
    "            # Make the discriminator network trainable\n",
    "            discriminator.trainable = True\n",
    "                    \n",
    "            # Create fake and real labels\n",
    "            labels_real = np.reshape([1] * batch_size, (-1, 1, 1, 1, 1))\n",
    "            labels_fake = np.reshape([0] * batch_size, (-1, 1, 1, 1, 1))\n",
    "                    \n",
    "            # Train the discriminator network\n",
    "            loss_real = discriminator.train_on_batch(volumes_batch, labels_real)\n",
    "            loss_fake = discriminator.train_on_batch(gen_volumes, labels_fake)\n",
    "                    \n",
    "            # Calculate total discriminator loss\n",
    "            d_loss = 0.5 * (loss_real + loss_fake)\n",
    "            z = np.random.normal(0, 0.33, size=[batch_size, 1, 1, 1, z_size]).astype(np.float32)\n",
    "\n",
    "            # Train the adversarial model\n",
    "            g_loss = adversarial_model.train_on_batch(z, np.reshape([1] * batch_size, (-1, 1, 1, 1, 1)))\n",
    "        \n",
    "            # Append the losses\n",
    "            gen_losses.append(g_loss)\n",
    "            dis_losses.append(d_loss)\n",
    "\n",
    "            # Generate and save the 3D images after each epoch\n",
    "            if index % 10 == 0:\n",
    "                z_sample2 = np.random.normal(0, 0.33, size=[batch_size, 1, 1, 1, z_size]).astype(np.float32)\n",
    "                generated_volumes = generator.predict(z_sample2, verbose=3)\n",
    "            \n",
    "            for i, generated_volume in enumerate(generated_volumes[:5]):\n",
    "                voxels = np.squeeze(generated_volume)\n",
    "                voxels[voxels < 0.5] = 0.\n",
    "                voxels[voxels >= 0.5] = 1.\n",
    "                saveFromVoxels(voxels, f'results/img_{epoch}_{index}_{i}')\n",
    "\n",
    "        # Save losses to Tensorboard\n",
    "        write_log(tensorboard, 'g_loss', np.mean(gen_losses), epoch)\n",
    "        write_log(tensorboard, 'd_loss', np.mean(dis_losses), epoch)\n",
    "\n",
    "    # Save the models\n",
    "    generator.save_weights(os.path.join(generated_volumes_dir, 'generator_weights.h5'))\n",
    "    discriminator.save_weights(os.path.join(generated_volumes_dir, 'discriminator_weights.h5'))\n",
    "\n",
    "    if MODE == 'predict':\n",
    "        # Create models\n",
    "        generator = build_generator()\n",
    "        discriminator = build_discriminator()\n",
    "\n",
    "        # Load model weights\n",
    "        generator.load_weights(os.path.join('models', 'generator_weights.h5'), True)\n",
    "        discriminator.load_weights(os.path.join('models', 'discriminator_weights.h5'), True)\n",
    "\n",
    "        # Generate 3D models\n",
    "        z_sample = np.random.normal(0, 1, size=[batch_size, 1, 1, 1, z_size]).astype(np.float32)\n",
    "        generated_volumes = generator.predict(z_sample, verbose=3)\n",
    "\n",
    "        for i, generated_volume in enumerate(generated_volumes[:2]):\n",
    "            voxels = np.squeeze(generated_volume)\n",
    "            voxels[voxels < 0.5] = 0.\n",
    "            voxels[voxels >= 0.5] = 1.\n",
    "            saveFromVoxels(voxels, 'results/gen_{}'.format(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "23dcebfce62aefae44217c5d4a070f68459ecc6c0c4cac8874d16e6b1ca5a009"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
